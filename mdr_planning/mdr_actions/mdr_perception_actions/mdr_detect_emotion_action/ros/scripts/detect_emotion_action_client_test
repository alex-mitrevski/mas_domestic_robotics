#! /usr/bin/env python
import sys

import rospy
import roslib
import actionlib

import cv2
from sensor_msgs.msg import Image

from mdr_detect_emotion_action.msg import DetectEmotionAction, DetectEmotionGoal, DetectEmotionFeedback, DetectEmotionResult

if __name__ == '__main__':
    rospy.init_node('detect_emotion_client_test')
    if len(sys.argv) != 2:
        print 'Usage: detect_emotion_action_client_test <input_image_path>'
    input_image_path = sys.argv[1]

    client = actionlib.SimpleActionClient('mdr_actions/detect_emotion_server', DetectEmotionAction)
    client.wait_for_server()

    goal = DetectEmotionGoal()

    img = cv2.imread(input_image_path)
    ros_image = Image()
    ros_image.height = img.shape[0]
    ros_image.width = img.shape[1]
    ros_image.encoding = 'bgr8'
    ros_image.data = img.flatten().tolist()
    goal.image = ros_image

    client.send_goal(goal)
    client.wait_for_result()
    print (client.get_result())
